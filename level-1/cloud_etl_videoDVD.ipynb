{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/meng-86/Big-Data-Homework---Alexa-can-you-handle-big-data/blob/main/level-1/cloud_etl_videoDVD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K2TVlJ-mDDlc",
    "outputId": "2bbd8f1b-b4c9-45d7-ee0d-63bebefd6bd4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'apt-get' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "The system cannot find the path specified.\n",
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "tar: Error opening archive: Failed to open '$SPARK_VERSION-bin-hadoop2.7.tgz'\n",
      "WARNING: Ignoring invalid distribution -cikit-learn (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cikit-learn (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cikit-learn (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cikit-learn (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cikit-learn (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cikit-learn (c:\\programdata\\anaconda3\\lib\\site-packages)\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Unable to find py4j in /content/spark-3.2.1-bin-hadoop2.7\\python, your SPARK_HOME may not be configured correctly",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\findspark.py\u001b[0m in \u001b[0;36minit\u001b[1;34m(spark_home, python_path, edit_rc, edit_profile)\u001b[0m\n\u001b[0;32m    158\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 159\u001b[1;33m             \u001b[0mpy4j\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mglob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspark_python\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"lib\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"py4j-*.zip\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    160\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_35172/2994054282.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;31m# Start a SparkSession\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfindspark\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m \u001b[0mfindspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\findspark.py\u001b[0m in \u001b[0;36minit\u001b[1;34m(spark_home, python_path, edit_rc, edit_profile)\u001b[0m\n\u001b[0;32m    159\u001b[0m             \u001b[0mpy4j\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mglob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspark_python\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"lib\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"py4j-*.zip\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 161\u001b[1;33m             raise Exception(\n\u001b[0m\u001b[0;32m    162\u001b[0m                 \"Unable to find py4j in {}, your SPARK_HOME may not be configured correctly\".format(\n\u001b[0;32m    163\u001b[0m                     \u001b[0mspark_python\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mException\u001b[0m: Unable to find py4j in /content/spark-3.2.1-bin-hadoop2.7\\python, your SPARK_HOME may not be configured correctly"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# Find the latest version of spark 3.0  from http://www.apache.org/dist/spark/ and enter as the spark version\n",
    "# For example:\n",
    "# spark_version = 'spark-3.0.3'\n",
    "spark_version = 'spark-3.2.1'\n",
    "os.environ['SPARK_VERSION']=spark_version\n",
    "\n",
    "# Install Spark and Java\n",
    "!apt-get update\n",
    "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
    "!wget -q http://www.apache.org/dist/spark/$SPARK_VERSION/$SPARK_VERSION-bin-hadoop2.7.tgz\n",
    "!tar xf $SPARK_VERSION-bin-hadoop2.7.tgz\n",
    "!pip install -q findspark\n",
    "\n",
    "# Set Environment Variables\n",
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = f\"/content/{spark_version}-bin-hadoop2.7\"\n",
    "\n",
    "# Start a SparkSession\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "08Lwnd0_DEml",
    "outputId": "23185bef-5e60-48ad-89dc-ac62ccdab119"
   },
   "outputs": [],
   "source": [
    "!wget https://jdbc.postgresql.org/download/postgresql-42.2.9.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9ph_hx_FFehl"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"CloudETL\").config(\"spark.driver.extraClassPath\",\"/content/postgresql-42.2.9.jar\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FKVDn-ZaFhXU",
    "outputId": "c8445e0a-9926-46f2-8235-bd3fa2fb3f49"
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkFiles\n",
    "url =\"https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Video_DVD_v1_00.tsv.gz\"\n",
    "spark.sparkContext.addFile(url)\n",
    "df = spark.read.csv(SparkFiles.get(\"amazon_reviews_us_Video_DVD_v1_00.tsv.gz\"), sep=\"\\t\", header=True)\n",
    "\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GTPai5GaGG6N"
   },
   "source": [
    "# Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ec_W8URKFnRE",
    "outputId": "08b39956-702f-4d2d-9901-5868edc8c98e"
   },
   "outputs": [],
   "source": [
    "# Drop missing values\n",
    "\n",
    "df = df.dropna()\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IpewiTeKGAcV",
    "outputId": "53ed3eba-d0dd-4b10-b1bd-b87107951de2"
   },
   "outputs": [],
   "source": [
    "# Count the data frame size\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TPBBJFawGLO0",
    "outputId": "eddaf9fd-bc8d-4f0b-b9a1-64cc7d0bc238"
   },
   "outputs": [],
   "source": [
    "# Print the Schema\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bhHS_nE4GpRs"
   },
   "source": [
    "# Transform Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lyu6cTgSGqME",
    "outputId": "99a1107b-3b48-45cf-d036-ef61c973ee8b"
   },
   "outputs": [],
   "source": [
    "# Data reviewing and table construction \n",
    "\n",
    "review_id_df = df.select([\"review_id\", \"customer_id\", \"product_id\", \"product_parent\", \"review_date\"])\n",
    "review_id_df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PlQGcXITGyul",
    "outputId": "f44c0e60-e0ee-472a-dfa1-51465f885d28"
   },
   "outputs": [],
   "source": [
    "# Drop/checking duplications by \"review_id\"\n",
    "review_id_df = review_id_df.dropDuplicates([\"review_id\"])\n",
    "review_id_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i4lAd0e1HV1F",
    "outputId": "1035655e-620a-4920-9b10-f62fff0bb460"
   },
   "outputs": [],
   "source": [
    "# Transform dataframe to fit review_id, print the schema\n",
    "\n",
    "from pyspark.sql.types import * \n",
    "\n",
    "review_id_df = review_id_df.withColumn(\"customer_id\",review_id_df[\"customer_id\"].cast(IntegerType()))\\\n",
    "    .withColumn(\"product_parent\",review_id_df[\"product_parent\"].cast(IntegerType()))\\\n",
    "    .withColumn(\"review_date\",review_id_df[\"review_date\"].cast(DateType()))\n",
    "\n",
    "review_id_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_o5x-mJkH2EF",
    "outputId": "aa73eb59-1445-4998-bd85-ba27b2c1347d"
   },
   "outputs": [],
   "source": [
    "# Transform dataframe to fit customer_id, print the schema\n",
    "customers_df = df.select([\"customer_id\"])\n",
    "customers_df.show(5)\n",
    "\n",
    "customers_df = customers_df.groupBy(\"customer_id\").count()\n",
    "customers_df.orderBy(\"customer_id\").select([\"customer_id\", \"count\"])\n",
    "customers_df.show(5)\n",
    "\n",
    "customers_df = customers_df.withColumnRenamed(\"count\", \"customer_count\")\n",
    "customers_df.show(5)\n",
    "\n",
    "customers_df = customers_df.withColumn(\"customer_id\",customers_df[\"customer_id\"].cast(IntegerType()))\\\n",
    ".withColumn(\"customer_count\",customers_df[\"customer_count\"].cast(IntegerType()))\n",
    "\n",
    "customers_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mtTJCUZ7H4C1",
    "outputId": "bd6e25b0-6d37-4e02-e050-bcd9334a31a7"
   },
   "outputs": [],
   "source": [
    "# Transform dataframe to product_id, print the schema\n",
    "products_df = df.select([\"product_id\", \"product_title\"])\n",
    "products_df.count()\n",
    "\n",
    "# Drop duplications \n",
    "products_df = products_df.dropDuplicates([\"product_id\"])\n",
    "products_df.count()\n",
    "\n",
    "# print the schema\n",
    "products_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SW4YfULEH6p1"
   },
   "outputs": [],
   "source": [
    "# Transform dataframe based on \"vine\", print the schema\n",
    "\n",
    "vine_df = df.select([\"review_id\",\"star_rating\", \"helpful_votes\", \"total_votes\", \"vine\"])\n",
    "vine_df.show(5)\n",
    "\n",
    "vine_df = vine_df.dropDuplicates([\"review_id\"])\n",
    "\n",
    "vine_df = vine_df.withColumn(\"star_rating\",vine_df[\"star_rating\"].cast(IntegerType()))\\\n",
    "    .withColumn(\"helpful_votes\",vine_df[\"helpful_votes\"].cast(IntegerType()))\\\n",
    "    .withColumn(\"total_votes\",vine_df[\"total_votes\"].cast(IntegerType()))\n",
    "\n",
    "vine_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-1AebgY2JHlN"
   },
   "source": [
    "# Write DataFrames to Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rg6_KvY5JHWF"
   },
   "outputs": [],
   "source": [
    "mode = \"append\"\n",
    "jdbc_url=\"jdbc:postgresql://mypostgresdb.cjpyragkbq4i.us-east-1.rds.amazonaws.com:5432/my_data_class_db\"\n",
    "config = {\"user\":\"root\", \n",
    "          \"password\": \"\", \n",
    "          \"driver\":\"org.postgresql.Driver\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gA_61PzbJLKl"
   },
   "outputs": [],
   "source": [
    "review_id_df.write.jdbc(url=jdbc_url, table=\"review_id_table\", mode=mode, properties=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QsdW3v6FJPmF"
   },
   "outputs": [],
   "source": [
    "customers_df.write.jdbc(url=jdbc_url, table=\"customers\", mode=mode, properties=config)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zOHwpZy6JQOF"
   },
   "outputs": [],
   "source": [
    "products_df.write.jdbc(url=jdbc_url, table=\"products\", mode=mode, properties=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aHWPErkwJS6l"
   },
   "outputs": [],
   "source": [
    "vine_df.write.jdbc(url=jdbc_url, table=\"vine_table\", mode=mode, properties=config)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNgyZual+cBkOmpHcXvQu4z",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "cloud_etl_videoDVD.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
